BTP:
Design decisions:
1. To fix initial g_c=0, added the point to rem_vec. Shouldn;t be a problem as if marg_vec is empty, the candidate in next round will move this point or push it further away.
    More importantly, it's alpha is 0 at this stage and it shouldn't be a problem.

PROBLEMS:
1. MORE??????? - the precision and calculations have become too sensitive
and are easily going crazy - ***NUMERIC INSTABILITY***
 CORRECTIONS:
 1. Add cor = 2*10**-6/C for each kernel calculation
 2. Add cor check for gamma of data point entering marg
 3. Add correction to R



-----------------------
COMPLETE CV

TODO:
1 Understand and implement the author's code
2. Add check that candidate was alrady added in handle_empty_marg,
    cur_check not good enough for initial g_c=0
3. Add iterator for getting data for tf instead of the old data loader
4. When you add classes, rememeber to check that labels are -1,1 and do appropriate scaling and
    scaling back


DISTRIB TF:
1. One of the potentially confusing things with Distributed TensorFlow is that very often, the same code will be sent to all nodes. So your main.py or train.py or whatever you choose to call your script will be sent to the workers and the ps. Environment variables are then used to execute a certain code block on the master node, another on the workers, another on the ps, etc


SUMMIT NOTES:
1. Tasks:
    i) PS tasks: hold variables and update them
    ii) Worker tasks: intensive calculations - 
2. Our code won;t be able to do data-parallelism
3. Partitioning variables - 13:13
4. Processes communicate through tf.Server - Need a cluster manager to manage the namespaces
5. ClusterSpec defines the set of processes/
6. Server represents task in the cluster
7. worker tasks create session, while ps tasks just wait using join
8. Fault tolerance through checkpointing and hooks


MORE NOTES:
1. jobs: random names - like (worker, ps) or just (local)
2. tasks: actual processes - hostname:port specifiy


COMPLETE CV
-----------------------------------------------------
24-03
1. Clean up the btp_python code
2. Test the same changes for tf - first float32 and then float64
    CHECK if there is a global setting to make default type of tf as f64
3. Possibly add indices as one of the variables

22-03
1. My code fails with certain test cases - singular matrix, infinite loop
2. The author's code seems to work for these data
3. Time to read and implement his code

20-03
1. Debug the test_code - based on initial g_c = 0 and 
    dumb mistake - forgot to make labels (1, -1) from 1, 0
2. Add dist code


19-03
----DONE----
1. Write code for prediction
2. Write code for saving
3. Write code for testing with a bunch of datasets

------------

4. Try adding distributed code

16-03
1. Error with grpc calls - fixed it by unsetting the http and https_proxy variables.

15-03
1. Try first in local: two tasks - one for gpu and another for cpu, talking btw processes is done by TF
2. Try ClusterManager: Currently, no direct support for any cluster manager and manual specifications of task is to be done - Good for me, I can blame TF :)




14-03
ERRORS:
1. alpha was C instead of 0 while calling from handle_empty_rem
2. Gamma_c in the calculation of R whlie addition of new data point needs to be recalulcated
wrt to beta_c of the point to be added 










-----------------------
PROBS:
1. frame error - None control dep
2. WHILE loop not getting executed again - some recomp problem - read carefully - loop_var namedtuple
3. Now error of while loop vars not being able to participate in any operations - cause - None control dep

Reproduce and fix 2


ERRORS:
1. control dependency -- None is causing the error of whlie loop tensors cant be used in any
operation
2.
------------------------------- PIVOT ---------------------------

--------PYTORCH--------
1. Broadcasting
2. CUDA - automatic synchronization - new operator to create tensor in the same 
dtype = torch.cuda.FloatTensor
for cuda, use dataparallel instead of multiprocessing
3. map function


